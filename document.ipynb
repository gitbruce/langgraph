{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import certifi\n",
    "\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "import truststore\n",
    "truststore.inject_into_ssl()\n",
    "\n",
    "import requests\n",
    "url = \"https://example.com\"\n",
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "tongyi_chat = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I like programming.', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 30, 'total_tokens': 34, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-632b0e8b-611c-402f-9264-eb3bdf567ed5-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"你是一名专业的翻译家，可以将用户的中文翻译为英文。\"),\n",
    "    (\"human\", \"我喜欢编程。\"),\n",
    "]\n",
    "tongyi_chat.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SentenceTransformers' from 'langchain.embeddings' (C:\\Users\\feng.z\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\embeddings\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DirectoryLoader\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformers\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ollama\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SentenceTransformers' from 'langchain.embeddings' (C:\\Users\\feng.z\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\embeddings\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from typing import Callable\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langgraph import LangGraph\n",
    "\n",
    "# Step 1: Abstract Component Interfaces\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = SentenceTransformer(model_name=model_name)\n",
    "\n",
    "    def generate_embeddings(self, documents):\n",
    "        return FAISS.from_documents(documents, self.embedding_model)\n",
    "\n",
    "class LocalLLM:\n",
    "    def __init__(self, model_name: str, host: str):\n",
    "        self.model_name = model_name\n",
    "        self.llm = Ollama(model=model_name, host=host)\n",
    "\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        return self.llm(prompt)\n",
    "\n",
    "\n",
    "# Step 2: Incremental Update Logic\n",
    "class KnowledgeLibrary:\n",
    "    def __init__(self, vectorstore_path: str, embedding_model: EmbeddingModel):\n",
    "        self.vectorstore_path = vectorstore_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vectorstore = self.load_vectorstore()\n",
    "\n",
    "    def load_vectorstore(self):\n",
    "        if os.path.exists(self.vectorstore_path):\n",
    "            return FAISS.load_local(self.vectorstore_path, self.embedding_model.embedding_model)\n",
    "        return None\n",
    "\n",
    "    def hash_document(self, content: str) -> str:\n",
    "        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def build_or_update(self, documents):\n",
    "        updated = False\n",
    "        hashes = set()\n",
    "        new_docs = []\n",
    "\n",
    "        # Load existing hashes\n",
    "        if self.vectorstore:\n",
    "            hashes = set(self.vectorstore.docstore.get_all_metadata())\n",
    "\n",
    "        # Check for new or updated documents\n",
    "        for doc in documents:\n",
    "            doc_hash = self.hash_document(doc.page_content)\n",
    "            if doc_hash not in hashes:\n",
    "                new_docs.append(doc)\n",
    "                updated = True\n",
    "\n",
    "        if updated:\n",
    "            new_store = self.embedding_model.generate_embeddings(new_docs)\n",
    "            if self.vectorstore:\n",
    "                self.vectorstore.merge_from(new_store)\n",
    "            else:\n",
    "                self.vectorstore = new_store\n",
    "            self.vectorstore.save_local(self.vectorstore_path)\n",
    "\n",
    "    def get_retriever(self):\n",
    "        return self.vectorstore.as_retriever() if self.vectorstore else None\n",
    "\n",
    "\n",
    "# Step 3: Workflow and Summarization\n",
    "def build_workflow(retriever, llm):\n",
    "    chain = ConversationalRetrievalChain.from_llm(llm.llm, retriever=retriever)\n",
    "    graph = LangGraph()\n",
    "    graph.add_node(\"retrieve_and_qa\", chain)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def summarize_domain_fsd(retriever, llm, domain: str) -> str:\n",
    "    documents = retriever.get_relevant_documents(domain)\n",
    "    summary = \"\\n\\n\".join([llm.generate_response(f\"Summarize this: {doc.page_content}\") for doc in documents])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Main Function\n",
    "def main():\n",
    "    # Paths and configurations\n",
    "    fsd_directory = \"./fsd_documents\"\n",
    "    vectorstore_path = \"./vectorstore/faiss_index\"\n",
    "    business_domain = \"Retail Lending\"\n",
    "\n",
    "    # Instantiate components\n",
    "    embedding_model = EmbeddingModel(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    llm = LocalLLM(model_name=\"llama\", host=\"http://localhost:11434\")\n",
    "    knowledge_library = KnowledgeLibrary(vectorstore_path=vectorstore_path, embedding_model=embedding_model)\n",
    "\n",
    "    # Load FSD documents\n",
    "    loader = DirectoryLoader(fsd_directory, glob=\"*.txt\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Build or update knowledge library\n",
    "    knowledge_library.build_or_update(documents)\n",
    "    retriever = knowledge_library.get_retriever()\n",
    "\n",
    "    if retriever:\n",
    "        # Build workflow\n",
    "        graph = build_workflow(retriever, llm)\n",
    "\n",
    "        # Summarize domain-specific FSDs\n",
    "        summary = summarize_domain_fsd(retriever, llm, business_domain)\n",
    "        print(f\"Summary for {business_domain}:\\n{summary}\")\n",
    "\n",
    "        # Example Q&A\n",
    "        query = \"What are the key requirements for retail lending?\"\n",
    "        answer = graph.run(\"retrieve_and_qa\", query=query)\n",
    "        print(f\"Q&A Result:\\n{answer}\")\n",
    "    else:\n",
    "        print(\"Knowledge library is empty. Please add documents.\")\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
